{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pilosvON1mmM"
   },
   "source": [
    "## Assignment 3: Image Classification with Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Group: Group 1<br>\n",
    "Name: Shaun Bisset, Qichun Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf4sdQhx1-5L"
   },
   "source": [
    "### Part 1: Building a Multiclass Classifier Based on the Fashion MNIST Dataset \n",
    "Main author: Shaun Bisset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYoHx7sC2K6z"
   },
   "source": [
    "1. Create a Jupyter notebook and import the necessary libraries. Import the Fashion MNIST dataset from TensorFlow using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Y2St7ikQM5a"
   },
   "outputs": [],
   "source": [
    "#Q1 import libraries\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#set base image parameters\n",
    "batch_size = 16\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk_1qvR32urz"
   },
   "source": [
    "2. Describe the dataset. How many images are there and in the training and testing sets? What size are they? Are the images colour, grayscale or black and white? List the values that the label can take. You can read more about this dataset on TensorFlow's website here: https://www.tensorflow.org/datasets/catalog/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glV2a8njQUOf"
   },
   "source": [
    "Q2 Describe data <br/>\n",
    "Train data consists of 60000 greyscale images 28px X 28px showing 10 different types of clothing including T-Shirt, Trousers, Pullover hoodie, Dress, Coat,Sandal, Shirt, Sneaker, Bag, Ankle boot. The test data set is the same types of images, color, size etc. though there are only 10000 of them. The label is a single integer from 0-9 each representing one of the clothing items listed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNpDe4Yu2y4L"
   },
   "source": [
    "3. Load the Fashion MNIST dataset using fashion_mnist.load_data() and save the results to (features_train, label_train), (features_test, label_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmfu4_8ZQRBo",
    "outputId": "49499f66-6d6c-43be-ef77-572eded00803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "26435584/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Q3 train test split\n",
    "(features_train, label_train), (features_test, label_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s84hVZOf2_RU"
   },
   "source": [
    "4. Print out the shape of features_train and features_test. Reshape these to add a new dimension corresponding to the number of colours, so that they are compatible with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfMxtG6jQZIM",
    "outputId": "fed4318f-f038-4d91-e329-89f6e117cfda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Feature list\n",
      " (60000, 28, 28)\n",
      "Test Feature list\n",
      " (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Q4 reshape to add 3th demention\n",
    "print('Train Feature list\\n',features_train.shape)\n",
    "print('Test Feature list\\n',features_test.shape)\n",
    "\n",
    "features_train = features_train.reshape(60000, 28, 28, 1)\n",
    "features_test = features_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DunO12Lg3EOb"
   },
   "source": [
    "5. Create a data generator with the following data augmentation for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XNiKsZSQdpB"
   },
   "outputs": [],
   "source": [
    "#Q5 creating training data generator\n",
    "dataGeneratorTrain = ImageDataGenerator(\n",
    "                                        rescale=1./255, \n",
    "                                        rotation_range=40, \n",
    "                                        width_shift_range=0.1, \n",
    "                                        height_shift_range=0.1, \n",
    "                                        shear_range=0.2, \n",
    "                                        zoom_range=0.2, \n",
    "                                        horizontal_flip=True, \n",
    "                                        fill_mode='nearest')\n",
    "\n",
    "dataGeneratorTrain = dataGeneratorTrain.flow(features_train, label_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1h8s1PN3-HD"
   },
   "source": [
    "6. Create another data generator for testing that only has rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxOKcA6cQhxY"
   },
   "outputs": [],
   "source": [
    "#Q6 creating testing data generator\n",
    "dataGeneratorTest = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "dataGeneratorTest = dataGeneratorTest.flow(features_train, label_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20Di918H4lAb"
   },
   "source": [
    "7. Create a neural network architecture with the following layers: a convolutional layer with Conv2D(64, (3,3), activation='relu') followed by MaxPooling2D(2,2); a convolutional layer with Conv2D(64, (3,3), activation='relu') followed by MaxPooling2D(2,2); a flatten layer; a fully connected layer with Dense(128, activation='relu'); a fully connected layer with Dense(10, activation='softmax')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKF85PiJQkDR"
   },
   "outputs": [],
   "source": [
    "#Q7 network architechture\n",
    "model = tf.keras.Sequential([\n",
    "                              layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                              layers.MaxPooling2D(2,2),\n",
    "                              layers.Conv2D(64, (3,3), activation='relu'),\n",
    "                              layers.MaxPooling2D(2,2),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(128, activation='relu'),\n",
    "                              layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrJ3S9Qj6Jz7"
   },
   "source": [
    "8. Compile the model using an Adam optimizer with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIdMv1aTQmwE"
   },
   "outputs": [],
   "source": [
    "#Q8 adamizer\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpUJVpX86Poj"
   },
   "source": [
    "9. Train the model and evaluate it on the testing set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkzIEG9yQqdI"
   },
   "outputs": [],
   "source": [
    "#Q9.1\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktL6WtcKQsnV",
    "outputId": "2930c29f-f87b-447a-fd88-fe8df5e6a723"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3750/3750 [==============================] - 138s 36ms/step - loss: 0.8150 - accuracy: 0.6960 - val_loss: 0.5832 - val_accuracy: 0.7729\n",
      "Epoch 2/5\n",
      "3750/3750 [==============================] - 135s 36ms/step - loss: 0.5986 - accuracy: 0.7737 - val_loss: 0.5034 - val_accuracy: 0.8099\n",
      "Epoch 3/5\n",
      "3750/3750 [==============================] - 134s 36ms/step - loss: 0.5377 - accuracy: 0.7974 - val_loss: 0.5521 - val_accuracy: 0.7984\n",
      "Epoch 4/5\n",
      "3750/3750 [==============================] - 132s 35ms/step - loss: 0.5026 - accuracy: 0.8103 - val_loss: 0.4139 - val_accuracy: 0.8400\n",
      "Epoch 5/5\n",
      "3750/3750 [==============================] - 133s 36ms/step - loss: 0.4753 - accuracy: 0.8218 - val_loss: 0.4010 - val_accuracy: 0.8447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dc460d090>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.2\n",
    "model.fit_generator(dataGeneratorTrain, steps_per_epoch=len(features_train) // batch_size, epochs=5, validation_data=dataGeneratorTest, validation_steps=len(features_test) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjruP3mmhgkP"
   },
   "source": [
    "An accuracy of 82% was acheived as per expected in the assignment instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E6reUmp7HQM"
   },
   "source": [
    "### Part 2: Fruit Classification with Transfer Learning\n",
    "Main author: Qichun Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLU0r-Ui7SPF"
   },
   "source": [
    "1. Create a new notebook and import the necessary libraries, including pathlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6uz7EVQ7N5_"
   },
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_knr1ua7a-j"
   },
   "source": [
    "2. Import the dataset and unzip the file using TensorFlow. Use the following code to download and unzip the Fruits360 dataset, and setup the training and validation paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLFPbWLo7Xuk",
    "outputId": "2ed07662-09be-4ed6-ed60-00adf347130e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip\n",
      "82223104/82220233 [==============================] - 0s 0us/step\n",
      "82231296/82220233 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# download from github, locate the fruits360_filtered and store in the path variable\n",
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip'\n",
    "zip_dir = tf.keras.utils.get_file('fruits360.zip', origin=file_url, extract=True)\n",
    "path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'\n",
    "train_dir = path / 'Training'\n",
    "validation_dir = path / 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am0fOBfE80tS"
   },
   "source": [
    "3. Describe the dataset. How many images are there and in the training and testing sets? What size are they? Are the images colour, grayscale or black and white?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bq4AbtGEDRQL",
    "outputId": "5c773b19-265f-435b-a4eb-8e85c5ff5f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fruits-360: A dataset of images containing fruits and vegetables\n",
      "\n",
      "## Version: 2019.09.21.0 ##\n",
      "\n",
      "A high-quality, dataset of images containing fruits and vegetables. The following fruits and vegetables are included: \n",
      "Apples (different varieties: Crimson Snow, Golden, Golden-Red, Granny Smith, Pink Lady, Red, Red Delicious), Apricot, Avocado, Avocado ripe, Banana (Yellow, Red, Lady Finger), Beetroot Red, Blueberry, Cactus fruit, Cantaloupe (2 varieties), Carambula, Cauliflower, Cherry (different varieties, Rainier), Cherry Wax (Yellow, Red, Black), Chestnut, Clementine, Cocos, Dates, Eggplant, Ginger Root, Granadilla, Grape (Blue, Pink, White (different varieties)), Grapefruit (Pink, White), Guava, Hazelnut, Huckleberry, Kiwi, Kaki, Kohlrabi, Kumsquats, Lemon (normal, Meyer), Lime, Lychee, Mandarine, Mango (Green, Red), Mangostan, Maracuja, Melon Piel de Sapo, Mulberry, Nectarine (Regular, Flat), Nut (Forest, Pecan), Onion (Red, White), Orange, Papaya, Passion fruit, Peach (different varieties), Pepino, Pear (different varieties, Abate, Forelle, Kaiser, Monster, Red, Williams), Pepper (Red, Green, Yellow), Physalis (normal, with Husk), Pineapple (normal, Mini), Pitahaya Red, Plum (different varieties), Pomegranate, Pomelo Sweetie, Potato (Red, Sweet, White), Quince, Rambutan, Raspberry, Redcurrant, Salak, Strawberry (normal, Wedge), Tamarillo, Tangelo, Tomato (different varieties, Maroon, Cherry Red, Yellow), Walnut.\n",
      "\n",
      "## Dataset properties ##\n",
      "\n",
      "Total number of images: 82213.\n",
      "\n",
      "Training set size: 61488 images (one fruit or vegetable per image).\n",
      "\n",
      "Test set size: 20622 images (one fruit or vegetable per image).\n",
      "\n",
      "Multi-fruits set size: 103 images (more than one fruit (or fruit class) per image)\n",
      "\n",
      "Number of classes: 120 (fruits and vegetables).\n",
      "\n",
      "Image size: 100x100 pixels.\n",
      "\n",
      "Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg) or r2_image_index_100.jpg or r3_image_index_100.jpg. \"r\" stands for rotated fruit. \"r2\" means that the fruit was rotated around the 3rd axis. \"100\" comes from image size (100x100 pixels).\n",
      "\n",
      "Different varieties of the same fruit (apple for instance) are stored as belonging to different classes.\n",
      "\n",
      "## Repository structure ##\n",
      "\n",
      "Folders [Training](Training) and [Test](Test) contain all images used for training and testing.\n",
      "\n",
      "Folder [test-multiple_fruits](test-multiple_fruits) contains images with multiple fruits. Some of them are partially covered by other fruits. Also, they were captured in different lighting conditions compared to the fruits from Training and Test folder. This is an excelent test for real-world detection.\n",
      "\n",
      "## How to cite ##\n",
      "\n",
      "Horea Muresan, [Mihai Oltean](https://mihaioltean.github.io), [Fruit recognition from images using deep learning](https://www.researchgate.net/publication/321475443_Fruit_recognition_from_images_using_deep_learning), Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.\n",
      "\n",
      "## How we created the dataset ##\n",
      "\n",
      "Fruits and vegetables were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded. \n",
      "\n",
      "A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.\n",
      "\n",
      "Behind the fruits we placed a white sheet of paper as background. \n",
      "\n",
      "However due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type: \n",
      "we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.\n",
      "\n",
      "All marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.\n",
      "\n",
      "The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.\n",
      "\n",
      "Pictures from the test-multiple_fruits folder were made with a Nexus 5X phone.\n",
      "\n",
      "## History ##\n",
      "\n",
      "Fruits were filmed at the dates given below (YYYY.MM.DD):\n",
      "\n",
      "2017.02.25 - Apple (golden).\n",
      "\n",
      "2017.02.28 - Apple (red-yellow, red, golden2), Kiwi, Pear, Grapefruit, Lemon, Orange, Strawberry.\n",
      "\n",
      "2017.03.05 - Apple (golden3, Braeburn, Granny Smith, red2).\n",
      "\n",
      "2017.03.07 - Apple (red3).\n",
      "\n",
      "2017.05.10 - Plum, Peach, Peach flat, Apricot, Nectarine, Pomegranate.\n",
      "\n",
      "2017.05.27 - Avocado, Papaya, Grape, Cherrie.\n",
      "\n",
      "2017.12.25 - Carambula, Cactus fruit, Granadilla, Kaki, Kumsquats, Passion fruit, Avocado ripe, Quince.\n",
      "\n",
      "2017.12.28 - Clementine, Cocos, Mango, Lime, Lychee.\n",
      "\n",
      "2017.12.31 - Apple Red Delicious, Pear Monster, Grape White.\n",
      "\n",
      "2018.01.14 - Banana, Grapefruit Pink, Mandarine, Pineapple, Tangelo.\n",
      "\n",
      "2018.01.19 - Huckleberry, Raspberry.\n",
      "\n",
      "2018.01.26 - Dates, Maracuja, Plum 2, Salak, Tamarillo.\n",
      "\n",
      "2018.02.05 - Guava, Grape White 2, Lemon Meyer\n",
      "\n",
      "2018.02.07 - Banana Red, Pepino, Pitahaya Red.\n",
      "\n",
      "2018.02.08 - Pear Abate, Pear Williams.\n",
      "\n",
      "2018.05.22 - Lemon rotated, Pomegranate rotated\n",
      "\n",
      "2018.05.24 - Cherry Rainier, Cherry 2, Strawberry Wedge.\n",
      "\n",
      "2018.05.26 - Cantaloupe (2 varieties).\n",
      "\n",
      "2018.05.31 - Melon Piel de Sapo.\n",
      "\n",
      "2018.06.05 - Pineapple Mini, Physalis, Physalis with Husk, Rambutan.\n",
      "\n",
      "2018.06.08 - Mulberry, Redcurrant.\n",
      "\n",
      "2018.06.16 - Cherry Red, Hazelnut, Walnut, Tomato.\n",
      "\n",
      "2018.06.17 - Cherry Wax (Yellow, Red, Black).\n",
      "\n",
      "2018.08.19 - Apple Red Yellow 2, Grape Blue, Grape White 3, Grape White 4, Peach 2, Plum 3, Tomato Maroon, Tomato 1-4.\n",
      "\n",
      "2018.12.20 - Nut Pecan, Pear Kaiser, Tomato Yellow.\n",
      "\n",
      "2018.12.21 - Banana Lady Finger, Chesnut, Mangostan.\n",
      "\n",
      "2018.12.22 - Pomelo Sweetie.\n",
      "\n",
      "2019.04.21 - Apple Crimson Snow, Apple Pink Lady, Blueberry, Kohlrabi, Mango Red, Pear Red, Pepper (Red, Yellow, Green).\n",
      "\n",
      "2019.06.18 - Beetroot Red, Ginger Root, Nectarine Flat, Nut Forest, Onion Red, Onion Red Peeled, Onion White, Potato Red, Potato Red Washed, Potato Sweet, Potato White.\n",
      "\n",
      "2019.07.07 - Cauliflower, Eggplant, Pear Forelle.\n",
      "\n",
      "## License ##\n",
      "\n",
      "MIT License\n",
      "\n",
      "Copyright (c) 2017-2019 [Mihai Oltean](https://mihaioltean.github.io), Horea Muresan\n",
      "\n",
      "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "of this software and associated documentation files (the \"Software\"), to deal\n",
      "in the Software without restriction, including without limitation the rights\n",
      "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
      "copies of the Software, and to permit persons to whom the Software is\n",
      "furnished to do so, subject to the following conditions:\n",
      "\n",
      "The above copyright notice and this permission notice shall be included in all\n",
      "copies or substantial portions of the Software.\n",
      "\n",
      "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
      "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
      "SOFTWARE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use read function to read readme file\n",
    "readme = path / 'fruits-360-readme.md'\n",
    "with open(readme) as readme:\n",
    "    print(readme.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_KYtwXu86AL"
   },
   "source": [
    "The Fruits-360 is an image dataset that contains fruits and vegetables (such as avocados, bananas, blueberry, cantaloupe, cherry and so on). The dataset has 120 classes of fruits and vegetables. From the fruits-360-readme.md file, the total number of images are 82,213. There are 61,488 images in the training sets and 20,622 images in the testing sets. The image size is 100 * 100 pixels. Images are colour with a white sheet of paper as a background. From the fruit-360 file that we downloaded, there are 11,398 images in the training folder and 4,752 images in the testing folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR31wPwK82cz"
   },
   "source": [
    "4. Create a data generator for 100 by 100 pixel images, with the following data augmentation for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MwZHcAB8uC0"
   },
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "train_image_generator = ImageDataGenerator(rescale=1./255, \n",
    "                                           rotation_range=40, \n",
    "                                           width_shift_range=0.1, \n",
    "                                           height_shift_range=0.1, \n",
    "                                           shear_range=0.2, \n",
    "                                           zoom_range=0.2, \n",
    "                                           horizontal_flip=True, \n",
    "                                           fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R_pZNvoFk9D"
   },
   "source": [
    "5. Create another data generator for testing 100 by 100 pixel images that only has rescaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyWt7jrlFiGB"
   },
   "outputs": [],
   "source": [
    "# Create data generator for testing\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lObuIF_EFqba"
   },
   "source": [
    "6. Load a pretrained VGG16 model from TensorFlow to be your base model. Specify weights='imagenet'and include_top=False. You will also need to know the image dimensions and number of channels (colours). Set this base model to be non-trainable using the .trainable attribute. Print a summary of the base model. How many parameters are there, and how many are trainable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTLR09GgFnr5",
    "outputId": "66d9db87-279b-4cf8-a958-c1fab1bcb246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# create vaiables to store values for batch_size, image height and width, and channel\n",
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width = 100\n",
    "channel = 3\n",
    "\n",
    "# Instantiate a VGG16 model\n",
    "base_model = VGG16(input_shape=(img_height, img_width, channel), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrfCcrrwKdez",
    "outputId": "3f1c0cf5-cc4c-4993-c097-01a688be89bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11398 images belonging to 120 classes.\n",
      "Found 4752 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a data generator called train_data_gen\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size, directory=train_dir, target_size=(img_height, img_width))\n",
    "# create a data generator called val_data_gen\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size, directory=validation_dir, target_size=(img_height, img_width))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhjpQ1HtKUKr"
   },
   "outputs": [],
   "source": [
    "# create two variables that will get the number of images for the training and validation sets\n",
    "total_train = 11398\n",
    "total_val = 4752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNuiyHhWGi1O"
   },
   "outputs": [],
   "source": [
    "# Set this model to non-trainable\n",
    "base_model.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0hxny1QGwCN",
    "outputId": "b2833834-fcd6-4fc1-9c15-5432caa83082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp1PtLXCG3oa"
   },
   "source": [
    "There are 14,714,688 parameters are there, but zero are trainable as we have frozen all the layers of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5SXdx2NHNha"
   },
   "source": [
    "7. Create a new model using tf.keras.Sequential() by adding the base model to the following layers: a flattening layer Flatten() , a fully connected layer Dense(1000, activation='relu') and a fully connected layer Dense(120, activation='softmax')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCq-ZvgfHMIa"
   },
   "outputs": [],
   "source": [
    "# create new model using Sequential\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation='relu'),\n",
    "    layers.Dense(120, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64AceuWuHbGa"
   },
   "source": [
    "8. Compile the model using an Adam optimizer with a learning rate of 0.001, categorical cross-entropy for the loss, and use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYuI4LGFG0pQ"
   },
   "outputs": [],
   "source": [
    "# instantiate using adam optimizer with a learning rate 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gy0X74k3HfgK"
   },
   "source": [
    "9. Train the model. Evaluate the model on the testing set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx63TQLLHdWx"
   },
   "outputs": [],
   "source": [
    "# compile the neural network model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfaNHnewHjii",
    "outputId": "b1058304-58d7-44fc-e41b-c29ae2b0129c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 3, 3, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4609000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               120120    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,443,808\n",
      "Trainable params: 4,729,120\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlNQPSEiHn3Z",
    "outputId": "25742fe8-9de7-4714-8af2-ea13419c6aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 1670s 2s/step - loss: 1.9965 - accuracy: 0.5166 - val_loss: 0.9953 - val_accuracy: 0.7376\n",
      "Epoch 2/5\n",
      "712/712 [==============================] - 1692s 2s/step - loss: 0.6607 - accuracy: 0.8074 - val_loss: 0.5755 - val_accuracy: 0.8260\n",
      "Epoch 3/5\n",
      "712/712 [==============================] - 1667s 2s/step - loss: 0.4201 - accuracy: 0.8715 - val_loss: 0.5731 - val_accuracy: 0.8359\n",
      "Epoch 4/5\n",
      "712/712 [==============================] - 1663s 2s/step - loss: 0.3341 - accuracy: 0.8935 - val_loss: 0.5427 - val_accuracy: 0.8590\n",
      "Epoch 5/5\n",
      "712/712 [==============================] - 1664s 2s/step - loss: 0.2825 - accuracy: 0.9104 - val_loss: 0.4005 - val_accuracy: 0.8761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dbf839690>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using fit_generator() to fit the neural network model with a epochs=5\n",
    "model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=total_train // batch_size,\n",
    "    epochs=5,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_steps=total_val // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hwU22PdH9xN"
   },
   "source": [
    "After training with five epochs, we achieved an accuracy score of 0.9104 for the training set and 0.8761 for the testing set. The score indicates the result of the training is remarkable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCBpF0sPI1fh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
